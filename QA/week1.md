| 问题 | 答案 |
|------|------|
| 1. 说一下 ChatGPT的优缺点 | ChatGPT优经过了预训练 微调以及强化学习。可以理解复杂的指令，完成问答任务。它的attention机制以及decoder的生成式的架构，在文本生成方面有很强能力。但是存在一些幻觉问题，同时上下文长度窗口有限，以及知识没有办法实时更新，甚至存在一些偏见。其次就是需要的计算资源很大。 |
| 2. 请简述下Transformer基本流程 | Transformer的架构由编码器和解码器组成，具体流程是这样的首先，文本经过分词，经过embedding转化为词嵌入向量，同时添加位置编码，进入encoder部分。<br>编码器部分由多个相同的层堆叠，每个子层包含了多头注意力机制以及前馈神经网络，同时也包含残差连接和规划，在多头注意力机制的内部：每一个token的向量都会注意到所有词，经过qkv的向量映射。分别得到查询q键k值v， Quarry和key的点积除以缩放因子。经soft max层获得权重，再与value值相乘求和，得到上下文的表征信息。<br>解码器部分同样由多个层堆叠，每个层包含三个子层为:掩码多头注意力、编码器解码器交叉注意力以及前馈神经网络。解码器使用掩码mask，使得每一个token只能注意到自己以及之前位置的token，一方面是为了模拟实际情况，同时也是充分的学习提取信息，交叉注意力层接收编码器提供的 k v值，通俗来说，编码器由于是全注意力，可以给解码器提供全面的参考信息，根据解码器本身带有的q值，与解码器的kv进行交叉注意力计算。利用解码器自有的信息与编码器的参考信息来预测下一个token。最后经过前馈神经网络层。<br>对于最后一层的输出处理，解码器通过全连接层的线性变换，将维度从向量维度映射到此表维度大小，并且使用soft max生成。每一个词的概率分布，最终根据策略选择，概率高的token输出 |
| 3. 为什么基于Transformer的架构需要多头注意力机制？ | 因为多头注意力机制可以捕获多个维度的信息，多个头可以允许模型关注到不同的表示子空间，增强信息的表达能力，同时也可以允许数据并行处理。 |
| 4. 编码器，解码器，编解码LLM模型之间的区别是什么？ | 编码器模型也就是encoder only，包含transformer的编码器部分，他使用的是双向注意力，上场语义理解类任务如文本分类情感分析等，代表的模型有Bert。解码器模型也就是disco的only，基于已知内容，自回归的生成文本，只包含transformer的解码器部分，属于单向注意力机制，更适合生成式任务，如文本生成续写，写作。编码器解码器模型。功能是将一个序列转化成另一个序列，同时包含编码器和解码器，编码器双向注意力，解码器单向注意力，适用于翻译摘要重写等， Sequence to sequence任务 |
| 5. 你能解释在语言模型中强化学习的概念吗？它如何应用于ChatGPT？ | ChatGPT经历预训练以及监督微调后。使用rlhf基于人类的强化学习dpo，对模型进行对齐，首先使用人工标注的数据（多label排序）训练奖励模型，奖励模型可以实现对某一问题某个答案的打分。在利用sft模型，针对每个问题生成多份答案，同时让奖励模型进行打分，让模型往更高的奖励上靠近。具体的流程如下： dpo中有4个主体， Inference、actor，reward、crical。 Actor是我们需要训练迭代的模型， Influence模型存在的意义是为了限制模型更新梯度不要太大，使用rl散度参与到损失函数计算中。评论家模型通过reward给到的奖励值，组合成优势函数，估计每轮推理得到的平均优势以及每一步的期望优势。迭代模型尽可能的增大优势。 |
| 6. 在GPT模型中，什么是温度系数？ | 温度系数是gpt类模型文本生成的一个采样超参数。它作用于最后一层的输出层，输出的逻辑词进行softmax之前，平均都除以温度系数，再进行指数计算。这样会改变原有的概率分布。<br>温度系数越低，原来概率大的词会变得更大，概率小的只会变得更小。分布会更加陡峭。采样的多样性会低。<br>相反温度系数高，分布会更平坦，采样多样性会更高 |
| 7. 什么是旋转位置编码（ROPE）？ | 旋转注意力编码是在，输入序列经 embedding词嵌入层后，利用向量的旋转变换， 可以改变词与词之间的位置想死滴关系，而不修改其值。它是一种相对位置编码，与传统位置编码具有更好的外推性，增强了长文本的处理能力。 |
| 8. 为什么现在的大模型大多是decoder-only的架构？ | Decoder架构适合自回归的生成式任务。 |
| 9. ChatGPT的训练步骤有哪些？ | 在gpt训练经过三步，第1步是预训练，使用大规模的数据，让模型参数学习语义信息。第2步有监督的指令微调，使用高质量的问答指令、回答对，让模型学会遵循指令的能力，第3步基于人类反馈的强化学习，是模型与人类的价值观对齐，回答更加准确。 |
|10为什么transformers需要位置编码？| 因为注意力机制本身是无序的，如果不采用位置编码，那不同token放在不同位置，所计算出的注意值是一样的，那么与实际不符。实际上来说，词序对语义是至关重要的。|
| 11. 如何缓解 LLMs 复读机问题？ |  增加训练数据的多样性、清洗高质量无大量重复的数据、提高模型推理采样温度、训练数据随机引入噪声。