### 统计类语言模型
语言模型的发展最开始是从统计类语言模型开始的。

但这种方式存在数据稀疏问题零频率并不代表0概率，尽管后面有一些平滑策略，但这不是根本解决方法。进而衍生出想通过向量之间的距离，来表示相关性。也就是神经语言模型nnlm。

### 神经语言模型
神经语言模型的初代，通过读热编码和词嵌入向量矩阵，拿到向量之后经过全连接与softmax得到概率。本质上任务是做预测，但出现一个副产物，也就是词嵌入向量W。发现这个矩阵可以用来表达词的含义，进而开启了词向量的研究。

### 词向量模型
Word2vec发展了两种训练模式， Cbow 与skip gram。前者是通过，上下文预测中间词，后者反之。从任务上来说，是训练W词向量矩阵。

但是出现了两个问题，一个问题是没有考虑序列信息，另一个问题是没办法处理多义词。

### RNN
第1个问题，对nnlm进行改进引入历史中间隐藏状态，发展出了rnn，及其变体lstm。
有效解决了序列信息的问题。第2个问题通过继续改进lstm，引入了双向、多层（ Elmo是两层）lstm的机制，也就得到了Elmo模型。

### Transformer
尽管如此，模型对于长上下文信息特征抽取一般，当时就有人提出了attention机制与transformer的框架。

此架构由向量嵌入、Encoder、Decoder、输出层组成。在编码与解码时，引入了查询、键、值三种权重，通过查询与键进行相似度计算，也就是注意力分配，最后与值相乘来处理，不同token之间的注意力信息。

进而又出现了， Transformer改变而来的Bert与gpt模型架构。 
### Bert
Bert可以说是 nlp发展中的集大成者，他引用了 Elmo的双向注意力思想， Cbow的训练思想，以及attention block的应用。尤其在处理，语义理解方面很有优势，这是由他的训练机制决定的。
### GPT
而gpt则改造了transformer的decoder，带掩码的注意力以及前馈网络层，多层堆叠而成，掩码注意力决定了它更适合生成式任务。

再后来，gpt123模型发展探讨了使用什么优化函数，以及怎么把模型迁移到多种任务，还有提高模型泛化性的能力，改进了很多训练逻辑。

### InstructGPT
InstructGPT论文揭示了这些，使用人工标注的数据对预算的模型，微调得到sft模型。利用sft模型进行多轮推理，人工排序标注来训练RM模型。在利用rm模型对sft模型进行ppo算法优化。

指引现阶段大模型训练的三阶段。预训练+指令微调sft+强化学习RL。

### deepseek
在走到今天的deepseek已经对gpt模型做了很大改进。为了扩展上下文，使用了旋转位置编码rope。为了节省资源且提高性能，使用了MLA、RMS、MOE、 Swiglu、MTP、无损负载均衡。训练算法方面使用GRPO。